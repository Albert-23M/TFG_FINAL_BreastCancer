{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique client IDs:  ['demd2841' 'demd108109' 'demd5792' ... 'demd47982' 'demd6347' 'demd7374']\n",
      "Number of unique client IDs:  6000\n",
      "Number of rows in the dataset:  47542\n",
      "Columns in the dataset:  Index(['client_id', 'status', 'site', 'study_id', 'serie_id', 'image_id',\n",
      "       'view', 'laterality', 'age', 'mark_id', 'lesion_id', 'conspicuity',\n",
      "       'x1', 'x2', 'y1', 'y2', 'pathologies', 'manufacturer', 'pixel_spacing',\n",
      "       'magnification_factor', 'implant', 'xmin_cropped', 'xmax_cropped',\n",
      "       'ymin_cropped', 'ymax_cropped'],\n",
      "      dtype='object')\n",
      "Number of rows in the dataset after filtering:  4318\n",
      "Unique client IDs:  ['demd2841' 'demd5792' 'demd115271' ... 'demd5935' 'demd7499' 'demd7374']\n",
      "Number of unique client IDs:  2264\n"
     ]
    }
   ],
   "source": [
    "def filter_optimam_csv(csv_path, csv_out_path):\n",
    "    \"\"\"\n",
    "    Filter the Optimam CSV file to only include relevant columns and rows.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Filter the DataFrame to only include rows where the columns x1, x2, y1, and y2 are not null\n",
    "    df = df.dropna(subset=['x1', 'x2', 'y1', 'y2'])\n",
    "\n",
    "    # Filter rows to include only rows where they contain '*mass*' in pathologies column\n",
    "    df = df[df['pathologies'].str.contains('mass', case=False, na=False)]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    df.to_csv(csv_out_path, index=False)\n",
    "\n",
    "\n",
    "filtered_df = filter_optimam_csv('/home/albert/datasets/optimam/optimam_dataset.csv', 'optimam_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 0\n",
      "Number of images not found: 0\n",
      "Number of images saved: 4318\n",
      "Nuevo CSV guardado en: /home/albert/research/retinanet/pytorch-retinanet_old_version/optimam_filtered_bbox_resized.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_and_resize_images(csv_path, images_root, output_root):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    num_errors = 0\n",
    "    not_found = 0\n",
    "    saved_images = 0\n",
    "\n",
    "    scale_x_list = []\n",
    "    scale_y_list = []\n",
    "    resized_width_list = []\n",
    "    resized_height_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        client_id = str(row['client_id'])\n",
    "        study_id = str(row['study_id'])\n",
    "        image_id = str(row['image_id'])\n",
    "\n",
    "        original_image_path = os.path.join(images_root, client_id, study_id, f\"{image_id}.png\")\n",
    "\n",
    "        if os.path.exists(original_image_path):\n",
    "            try:\n",
    "                img = Image.open(original_image_path)\n",
    "                original_width, original_height = img.width, img.height\n",
    "\n",
    "                # Redimensionamos manteniendo aspecto\n",
    "                aspect_ratio = original_width / original_height\n",
    "                if original_width > original_height:\n",
    "                    new_width = 900\n",
    "                    new_height = int(new_width / aspect_ratio)\n",
    "                else:\n",
    "                    new_height = 900\n",
    "                    new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "                # Calculamos escalado\n",
    "                scale_x = new_width / original_width\n",
    "                scale_y = new_height / original_height\n",
    "\n",
    "                img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Guardamos la imagen redimensionada\n",
    "                output_dir = os.path.join(output_root, client_id, study_id)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_path = os.path.join(output_dir, f\"{image_id}.png\")\n",
    "                img_resized.save(output_path)\n",
    "                saved_images += 1\n",
    "\n",
    "                # Guardamos escalados\n",
    "                scale_x_list.append(scale_x)\n",
    "                scale_y_list.append(scale_y)\n",
    "                resized_width_list.append(new_width)\n",
    "                resized_height_list.append(new_height)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {original_image_path}: {e}\")\n",
    "                num_errors += 1\n",
    "                scale_x_list.append(None)\n",
    "                scale_y_list.append(None)\n",
    "                resized_width_list.append(None)\n",
    "                resized_height_list.append(None)\n",
    "        else:\n",
    "            print(f\"Imagen no encontrada: {original_image_path}\")\n",
    "            not_found += 1\n",
    "            scale_x_list.append(None)\n",
    "            scale_y_list.append(None)\n",
    "            resized_width_list.append(None)\n",
    "            resized_height_list.append(None)\n",
    "\n",
    "    print(f\"Number of errors: {num_errors}\")\n",
    "    print(f\"Number of images not found: {not_found}\")\n",
    "    print(f\"Number of images saved: {saved_images}\")\n",
    "\n",
    "    # Añadimos columnas nuevas al dataframe para los escalados de los bbox\n",
    "    df['scale_x'] = scale_x_list\n",
    "    df['scale_y'] = scale_y_list\n",
    "    df['resized_width'] = resized_width_list\n",
    "    df['resized_height'] = resized_height_list\n",
    "\n",
    "    new_csv_path = os.path.splitext(csv_path)[0] + '_bbox_resized.csv'\n",
    "    df.to_csv(new_csv_path, index=False)\n",
    "    print(f\"Nuevo CSV guardado en: {new_csv_path}\")\n",
    "\n",
    "csv_path = '/home/albert/research/retinanet/pytorch-retinanet_old_version/optimam_filtered.csv'\n",
    "images_root = '/home/kaisar/Datasets/OPTIMAM/images'\n",
    "output_root = '/home/albert/datasets/optimam/images'\n",
    "\n",
    "filter_and_resize_images(csv_path, images_root, output_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imágenes copiadas a /home/albert/datasets/optimam/images_flat\n"
     ]
    }
   ],
   "source": [
    "def copiar_imagenes_a_directorio_plano(csv_path, images_root, output_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        client_id = str(row[\"client_id\"])\n",
    "        study_id = str(row[\"study_id\"])\n",
    "        image_id = str(row[\"image_id\"])\n",
    "\n",
    "        # Ruta original esperada\n",
    "        image_filename = f\"{image_id}.png\"\n",
    "        original_path = os.path.join(images_root, client_id, study_id, image_filename)\n",
    "\n",
    "        # Nombre nuevo de imagen\n",
    "        new_filename = f\"{client_id}_{study_id}_{image_id}.png\"\n",
    "        output_path = os.path.join(output_dir, new_filename)\n",
    "\n",
    "        if os.path.exists(original_path):\n",
    "            shutil.copy2(original_path, output_path)\n",
    "        else:\n",
    "            print(f\"Imagen no encontrada: {original_path}\")\n",
    "\n",
    "    print(f\"Imágenes copiadas a {output_dir}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "copiar_imagenes_a_directorio_plano(\n",
    "    csv_path = '/home/albert/research/retinanet/pytorch-retinanet_old_version/optimam_filtered_bbox_resized.csv',\n",
    "    images_root=\"/home/albert/datasets/optimam/images\",\n",
    "    output_dir=\"/home/albert/datasets/optimam/images_flat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO JSON generado con 4136 imágenes y 4318 anotaciones.\n"
     ]
    }
   ],
   "source": [
    "def generate_coco_json(csv_path, images_dir, output_json_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    annotation_id = 0\n",
    "    image_id_map = {}\n",
    "    image_id_counter = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        client_id = str(row['client_id'])\n",
    "        study_id = str(row['study_id'])\n",
    "        image_id_str = str(row['image_id'])\n",
    "        dataset_id = client_id\n",
    "\n",
    "        file_name = f\"{client_id}_{study_id}_{image_id_str}.png\"\n",
    "        file_path = os.path.join(images_dir, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Imagen no encontrada: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Resized dimensions\n",
    "        width = int(row['resized_width'])\n",
    "        height = int(row['resized_height'])\n",
    "\n",
    "        scale_x = row['scale_x']\n",
    "        scale_y = row['scale_y']\n",
    "\n",
    "        # Bounding box original\n",
    "        xmin = float(row['x1'])\n",
    "        xmax = float(row['x2'])\n",
    "        ymin = float(row['y1'])\n",
    "        ymax = float(row['y2'])\n",
    "\n",
    "        # Bounding box escalado\n",
    "        bbox_x = xmin * scale_x\n",
    "        bbox_y = ymin * scale_y\n",
    "        bbox_w = (xmax - xmin) * scale_x\n",
    "        bbox_h = (ymax - ymin) * scale_y\n",
    "        area = bbox_w * bbox_h\n",
    "\n",
    "        # Añadimos imagen si aún no está registrada\n",
    "        unique_key = (client_id, study_id, image_id_str)\n",
    "        if unique_key not in image_id_map:\n",
    "            image_info = {\n",
    "                \"id\": image_id_counter,\n",
    "                \"Dataset_ID\": dataset_id,\n",
    "                \"file_name\": file_name,\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            }\n",
    "            images.append(image_info)\n",
    "            image_id_map[unique_key] = image_id_counter\n",
    "            image_id_counter += 1\n",
    "\n",
    "        # Mapear pathologies a category_id 1\n",
    "        category_id = 1\n",
    "\n",
    "        annotation = {\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_id_map[unique_key],\n",
    "            \"category_id\": category_id,\n",
    "            \"bbox\": [bbox_x, bbox_y, bbox_w, bbox_h],\n",
    "            \"type\": \"mass\",\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": 0\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "        annotation_id += 1\n",
    "\n",
    "    categories = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"ROI\",\n",
    "            \"supercategory\": \"mass_MALIGNANT\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"ROI\",\n",
    "            \"supercategory\": \"mass_BENIGN\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"name\": \"ROI\",\n",
    "            \"supercategory\": \"mass_BENIGN_WITHOUT_CALLBACK\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    coco_format = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "\n",
    "    print(f\"COCO JSON generado con {len(images)} imágenes y {len(annotations)} anotaciones.\")\n",
    "\n",
    "generate_coco_json(\n",
    "    csv_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/optimam_filtered_bbox_resized.csv',\n",
    "    images_dir='/home/albert/datasets/optimam/images_flat',\n",
    "    output_json_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/coco_optimam.json'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2648 imágenes en train\n",
      "✅ 661 imágenes en val\n",
      "✅ 827 imágenes en test\n"
     ]
    }
   ],
   "source": [
    "def dividir_y_guardar_imagenes(images_flat_dir, dest_root):\n",
    "    # Obtener todas las imágenes\n",
    "    imagenes = [f for f in os.listdir(images_flat_dir) if f.endswith('.png')]\n",
    "    random.shuffle(imagenes)\n",
    "\n",
    "    total = len(imagenes)\n",
    "    test_size = int(0.20 * total)\n",
    "    val_size = int(0.20 * (total - test_size))  # 20% del 80% restante\n",
    "\n",
    "    test_imgs = imagenes[:test_size]\n",
    "    val_imgs = imagenes[test_size:test_size + val_size]\n",
    "    train_imgs = imagenes[test_size + val_size:]\n",
    "\n",
    "    # Rutas destino\n",
    "    dirs = {\n",
    "        \"train\": os.path.join(dest_root, \"optimam_coco_mass_train\"),\n",
    "        \"val\": os.path.join(dest_root, \"optimam_coco_mass_val\"),\n",
    "        \"test\": os.path.join(dest_root, \"optimam_coco_mass_test\")\n",
    "    }\n",
    "\n",
    "    # Crear carpetas si no existen\n",
    "    for path in dirs.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Función para copiar\n",
    "    def copiar(lista, destino):\n",
    "        for img in lista:\n",
    "            src = os.path.join(images_flat_dir, img)\n",
    "            dst = os.path.join(destino, img)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "    # Copiar imágenes\n",
    "    copiar(train_imgs, dirs[\"train\"])\n",
    "    copiar(val_imgs, dirs[\"val\"])\n",
    "    copiar(test_imgs, dirs[\"test\"])\n",
    "\n",
    "    print(f\"✅ {len(train_imgs)} imágenes en train\")\n",
    "    print(f\"✅ {len(val_imgs)} imágenes en val\")\n",
    "    print(f\"✅ {len(test_imgs)} imágenes en test\")\n",
    "\n",
    "# Uso\n",
    "dividir_y_guardar_imagenes(\n",
    "    images_flat_dir=\"/home/albert/datasets/optimam/images_flat\",\n",
    "    dest_root=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado COCO para split en: /home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/instances_optimam_coco_mass_train.json\n",
      "   Imágenes: 2648 | Anotaciones: 2762\n",
      "✅ Guardado COCO para split en: /home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/instances_optimam_coco_mass_val.json\n",
      "   Imágenes: 661 | Anotaciones: 693\n",
      "✅ Guardado COCO para split en: /home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/instances_optimam_coco_mass_test.json\n",
      "   Imágenes: 827 | Anotaciones: 863\n"
     ]
    }
   ],
   "source": [
    "def generar_coco_split(coco_global_path, images_split_dir, output_json_path):\n",
    "    with open(coco_global_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    split_filenames = set(os.listdir(images_split_dir))\n",
    "\n",
    "    # Filtramos las imágenes del COCO que están en este split\n",
    "    imagenes_split = [\n",
    "        img for img in coco_data[\"images\"]\n",
    "        if img[\"file_name\"] in split_filenames\n",
    "    ]\n",
    "\n",
    "    # Obtenemos los IDs de imagenes seleccionadas\n",
    "    split_image_ids = set(img[\"id\"] for img in imagenes_split)\n",
    "\n",
    "    # Filtramos las anotaciones correspondientes\n",
    "    anotaciones_split = [\n",
    "        ann for ann in coco_data[\"annotations\"]\n",
    "        if ann[\"image_id\"] in split_image_ids\n",
    "    ]\n",
    "\n",
    "    # Mantenemos las categorías igual\n",
    "    categorias = coco_data[\"categories\"]\n",
    "\n",
    "    # Contruimos el json final\n",
    "    coco_split = {\n",
    "        \"images\": imagenes_split,\n",
    "        \"annotations\": anotaciones_split,\n",
    "        \"categories\": categorias\n",
    "    }\n",
    "\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_split, f, indent=4)\n",
    "\n",
    "    print(f\"Guardado COCO para split en: {output_json_path}\")\n",
    "    print(f\"Imágenes: {len(imagenes_split)} | Anotaciones: {len(anotaciones_split)}\")\n",
    "\n",
    "\n",
    "generar_coco_split(\n",
    "    coco_global_path=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/coco_optimam.json\",\n",
    "    images_split_dir=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/images/optimam_coco_mass_train\",\n",
    "    output_json_path=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/instances_optimam_coco_mass_train.json\"\n",
    ")\n",
    "\n",
    "generar_coco_split(\n",
    "    coco_global_path=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/coco_optimam.json\",\n",
    "    images_split_dir=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/images/optimam_coco_mass_val\",\n",
    "    output_json_path=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/instances_optimam_coco_mass_val.json\"\n",
    ")\n",
    "\n",
    "generar_coco_split(\n",
    "    coco_global_path=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/coco_optimam.json\",\n",
    "    images_split_dir=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/images/optimam_coco_mass_test\",\n",
    "    output_json_path=\"/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/annotations/instances_optimam_coco_mass_test.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campo 'segmentation' eliminado correctamente. Archivo guardado en: ./coco/annotations/instances_coco_mass_val_no_segmentation.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de TPR\\FPPI con Min of the Max FPPI y original\n",
    "\n",
    "os.makedirs('resultados_metricas', exist_ok=True)\n",
    "csv_files = glob.glob('resultados_metricas/*.csv')\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "all_max_fppi = []\n",
    "\n",
    "for file in csv_files:\n",
    "    if \"to\" in file:\n",
    "        continue\n",
    "    df = pd.read_csv(file)\n",
    "    label = os.path.splitext(os.path.basename(file))[0]\n",
    "    all_max_fppi.append(df['FPPI'].max())\n",
    "\n",
    "    plt.plot(df['FPPI'], df['TPR'], label=label)\n",
    "\n",
    "\n",
    "plt.xlabel('FPPI')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('Curvas FROC experimentos sin Transfer Learning')\n",
    "plt.grid(True)\n",
    "plt.legend(title_fontsize='small', loc='lower right', fontsize='small')\n",
    "plt.xlim(0, min(all_max_fppi))\n",
    "# plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura en carpeta\n",
    "# plt.savefig('resultados_metricas/froc_curvas_original_all.png', dpi=300)\n",
    "plt.savefig('resultados_metricas/froc_curvas_xLimited_to_min_of_max_fppi_no_TL.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para graficar las curvas FROC con FPPI normalizado y AUC desde el primer FPPI mayor que 0\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "os.makedirs('resultados_metricas', exist_ok=True)\n",
    "csv_files = glob.glob('resultados_metricas/*.csv')\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "all_max_fppi = []\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    if \"to\" not in file:\n",
    "        continue\n",
    "    df = pd.read_csv(file)\n",
    "    all_max_fppi.append(df['FPPI'].max())\n",
    "    dfs.append((os.path.splitext(os.path.basename(file))[0], df))\n",
    "\n",
    "min_max_fppi = min(all_max_fppi)\n",
    "\n",
    "for label, df in dfs:\n",
    "    df_filtered = df[df['FPPI'] <= min_max_fppi].copy()\n",
    "    df_filtered['FPPI_norm'] = df_filtered['FPPI'] / min_max_fppi\n",
    "\n",
    "    # Ordenamos por FPPI normalizado\n",
    "    df_filtered = df_filtered.sort_values('FPPI_norm')\n",
    "\n",
    "    # Encontramos el primer FPPI > 0\n",
    "    df_nonzero = df_filtered[df_filtered['FPPI_norm'] > 0]\n",
    "    if not df_nonzero.empty:\n",
    "        first_fppi = df_nonzero['FPPI_norm'].iloc[0]\n",
    "        first_tpr = df_nonzero['TPR'].iloc[0]\n",
    "\n",
    "        auc_x = [first_fppi] + list(df_nonzero['FPPI_norm'])\n",
    "        auc_y = [0] + list(df_nonzero['TPR'])\n",
    "\n",
    "        auc_score = auc(auc_x, auc_y)\n",
    "    else:\n",
    "        auc_score = 0.0  # No hay FPPI > 0\n",
    "\n",
    "    plt.plot(df_filtered['FPPI_norm'], df_filtered['TPR'], label=f\"{label} (AUC={auc_score:.3f})\")\n",
    "\n",
    "plt.xlabel('FPPI')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('Curvas FROC experimentos con Transfer Learning')\n",
    "plt.grid(True)\n",
    "plt.legend(title_fontsize='small', loc='lower right', fontsize='small')\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('resultados_metricas/froc_curvas_fppi_normalizado_auc_from_first_positive_TL.png', dpi=300)\n",
    "# plt.savefig('resultados_metricas/froc_curvas_fppi_normalizado_auc_from_first_positive_no_TL.png', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR interpolado en FPPI específicos:\n",
      "\n",
      "Modelo: CBIS-DDSM_TL_to_OPTIMAM\n",
      "  FPPI=0.0 → TPR=0.000\n",
      "  FPPI=0.5 → TPR=0.794\n",
      "  FPPI=0.7 → TPR=0.810\n",
      "  FPPI=1.0 → TPR=0.810\n",
      "\n",
      "Modelo: OPTIMAM_to_CBIS-DDSM\n",
      "  FPPI=0.0 → TPR=0.000\n",
      "  FPPI=0.5 → TPR=0.602\n",
      "  FPPI=0.7 → TPR=0.652\n",
      "  FPPI=1.0 → TPR=0.677\n",
      "\n",
      "Modelo: CBIS-DDSM_to_OPTIMAM\n",
      "  FPPI=0.0 → TPR=0.000\n",
      "  FPPI=0.5 → TPR=0.661\n",
      "  FPPI=0.7 → TPR=0.674\n",
      "  FPPI=1.0 → TPR=0.674\n",
      "\n",
      "Modelo: CBIS-DDSM_SI_DA_SI_Pretrained\n",
      "  FPPI=0.0 → TPR=0.000\n",
      "  FPPI=0.5 → TPR=0.638\n",
      "  FPPI=0.7 → TPR=0.669\n",
      "  FPPI=1.0 → TPR=0.682\n",
      "\n",
      "Modelo: CBIS-DDSM_NO_DA_NO_Pretrained\n",
      "  FPPI=0.0 → TPR=0.006\n",
      "  FPPI=0.5 → TPR=0.365\n",
      "  FPPI=0.7 → TPR=0.429\n",
      "  FPPI=1.0 → TPR=0.465\n",
      "\n",
      "Modelo: OPTIMAM\n",
      "  FPPI=0.0 → TPR=0.000\n",
      "  FPPI=0.5 → TPR=0.817\n",
      "  FPPI=0.7 → TPR=0.817\n",
      "  FPPI=1.0 → TPR=0.817\n",
      "\n",
      "Modelo: CBIS-DDSM_NO_DA_SI_Pretrained\n",
      "  FPPI=0.0 → TPR=0.000\n",
      "  FPPI=0.5 → TPR=0.632\n",
      "  FPPI=0.7 → TPR=0.674\n",
      "  FPPI=1.0 → TPR=0.688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para imprimir TPR interpolado en FPPI específicos desde los CSVs de resultados\n",
    "\n",
    "os.makedirs('resultados_metricas', exist_ok=True)\n",
    "csv_files = glob.glob('resultados_metricas/*.csv')\n",
    "\n",
    "# Puntos de FPPI que queremos analizar\n",
    "target_fppi = [0.0, 0.5, 0.7, 1.0]\n",
    "\n",
    "print(\"TPR interpolado en FPPI específicos:\\n\")\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    label = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "    # Asegurarse de que los datos estén ordenados por FPPI\n",
    "    df_sorted = df.sort_values(by='FPPI')\n",
    "\n",
    "    # Interpolación\n",
    "    interpolated_tpr = np.interp(target_fppi, df_sorted['FPPI'], df_sorted['TPR'])\n",
    "\n",
    "    print(f\"Modelo: {label}\")\n",
    "    for fppi_val, tpr_val in zip(target_fppi, interpolated_tpr):\n",
    "        print(f\"  FPPI={fppi_val:.1f} → TPR={tpr_val:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escalado de bounding boxes:\n",
      "Escalado X: [0.16805697 0.21965144 0.2703125  0.29365079 0.2202524  0.39184953\n",
      " 0.29296875 0.25497866 0.25571429 0.32132565]\n",
      "863 bounding boxes en el set de test\n",
      "0     222.339369\n",
      "9      75.340445\n",
      "12     53.610173\n",
      "14     33.518750\n",
      "18     73.795313\n",
      "20     71.632812\n",
      "21     37.843750\n",
      "22     47.575000\n",
      "23     37.573438\n",
      "24    103.800000\n",
      "dtype: float64\n",
      "\n",
      "0     254.621849\n",
      "9      66.796875\n",
      "12     48.235294\n",
      "14     29.206731\n",
      "18     74.639423\n",
      "20     57.331731\n",
      "21     46.243990\n",
      "22     38.671875\n",
      "23     25.420673\n",
      "24    109.254808\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def graficar_bbox_dimensions(csv_path, test_images_dir, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df['file_name'] = df.apply(lambda row: f\"{row['client_id']}_{row['study_id']}_{row['image_id']}.png\", axis=1)\n",
    "\n",
    "    # Filtramos por imágenes en el set de test\n",
    "    test_filenames = set(os.listdir(test_images_dir))\n",
    "    df_test = df[df['file_name'].isin(test_filenames)]\n",
    "\n",
    "    # Calculamos bbox width y height escalados\n",
    "    bbox_w = (df_test['x2'] - df_test['x1']) * df_test['scale_x']\n",
    "    bbox_h = (df_test['y2'] - df_test['y1']) * df_test['scale_y']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(bbox_w, bbox_h, alpha=0.6, edgecolors='k')\n",
    "    plt.xlabel('Bounding Box Width (px)')\n",
    "    plt.ylabel('Bounding Box Height (px)')\n",
    "    plt.title('Distribución de tamaños de Bounding Boxes (Test Set)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Crear carpeta de salida si no existe\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Guardar figura\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Uso\n",
    "graficar_bbox_dimensions(\n",
    "    csv_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/optimam_filtered_bbox_resized.csv',\n",
    "    test_images_dir='/home/albert/research/retinanet/pytorch-retinanet_old_version/coco/images/optimam_coco_mass_test',\n",
    "    output_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/resultados_metricas/test_image_size_distribution.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para graficar la distribución de tamaños de BBoxes correctos vs incorrectos\n",
    "\n",
    "def graficar_bbox_correct_vs_wrong(correct_csv_path, wrong_csv_path, output_path):\n",
    "    df_correct = pd.read_csv(correct_csv_path)\n",
    "    df_wrong = pd.read_csv(wrong_csv_path)\n",
    "\n",
    "    # Calculamos las dimensiones escaladas\n",
    "    w_correct = df_correct['width'] * df_correct['scale']\n",
    "    h_correct = df_correct['height'] * df_correct['scale']\n",
    "\n",
    "    w_wrong = df_wrong['width'] * df_wrong['scale']\n",
    "    h_wrong = df_wrong['height'] * df_wrong['scale']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(w_correct, h_correct, color='blue', alpha=0.6, edgecolors='k',\n",
    "                label=f'TP ({len(df_correct)})')\n",
    "    plt.scatter(w_wrong, h_wrong, color='red', alpha=0.6, edgecolors='k',\n",
    "                label=f'FN ({len(df_wrong)})')\n",
    "\n",
    "    plt.xlabel('Bounding Box Width (px)')\n",
    "    plt.ylabel('Bounding Box Height (px)')\n",
    "    plt.title('Distribución de tamaños de BBoxes - TP vs FN')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "graficar_bbox_correct_vs_wrong(\n",
    "    correct_csv_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/gt_pred_correct.csv',\n",
    "    wrong_csv_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/gt_pred_incorrect.csv',\n",
    "    output_path='/home/albert/research/retinanet/pytorch-retinanet_old_version/resultados_metricas/bbox_size_distribution_comparison.png'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
